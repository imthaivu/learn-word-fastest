import re
import fitz  # pip install pymupdf
import pandas as pd
import nltk
from nltk.stem import WordNetLemmatizer
from tqdm import tqdm  # pip install tqdm

# === Chuáº©n bá»‹ ===
nltk.download('wordnet', quiet=True)
lemmatizer = WordNetLemmatizer()

def normalize_phrasal(phrase: str) -> str:
    words = phrase.split()
    if not words:
        return phrase
    base = lemmatizer.lemmatize(words[0], 'v')
    return " ".join([base] + words[1:])

# === ÄÆ°á»ng dáº«n file ===
pdf_path = "LR.pdf"
phrasal_verbs_path = "phrasalverbFullType.txt"
output_path = "output.xlsx"

print("ğŸš€ Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh xá»­ lÃ½...")

# === Äá»c danh sÃ¡ch phrasal verbs Ä‘Ã£ má»Ÿ rá»™ng ===
print("ğŸ“¥ Äang Ä‘á»c danh sÃ¡ch phrasal verbs...")
with open(phrasal_verbs_path, "r", encoding="utf-8") as f:
    phrasal_lines = [line.strip().lower().replace("'", "") for line in f if line.strip()]
phrasal_set = set(phrasal_lines)
print(f"âœ… ÄÃ£ táº£i {len(phrasal_set)} cá»¥m tá»« tá»« {phrasal_verbs_path}")

# Sáº¯p xáº¿p theo Ä‘á»™ dÃ i giáº£m dáº§n Ä‘á»ƒ Æ°u tiÃªn cá»¥m dÃ i trÆ°á»›c
phrasal_sorted = sorted(phrasal_set, key=lambda x: -len(x.split()))

# === TrÃ­ch xuáº¥t vÄƒn báº£n tá»« PDF ===
print(f"ğŸ“„ Äang trÃ­ch xuáº¥t vÄƒn báº£n tá»« {pdf_path}...")
doc = fitz.open(pdf_path)
text = " ".join([page.get_text() for page in doc])
text = text.lower().replace("â€™", "'")
print("âœ… HoÃ n táº¥t trÃ­ch xuáº¥t vÃ  chuáº©n hÃ³a vÄƒn báº£n.")

# === Thay tháº¿ phrasal verbs báº±ng token Ä‘áº·c biá»‡t ===
print("ğŸ” Äang thay tháº¿ phrasal verbs báº±ng token (cÃ³ tiáº¿n trÃ¬nh)...")
placeholder_map = {}
token_id = 0

for phrasal in tqdm(phrasal_sorted, desc="Äang xá»­ lÃ½ phrasal verbs"):
    pattern = r'(?<!\w)' + re.escape(phrasal) + r'(?!\w)'
    if re.search(pattern, text):
        token = f"__PHV{token_id}__"
        text = re.sub(pattern, token, text)
        placeholder_map[token] = phrasal
        token_id += 1

print(f"âœ… ÄÃ£ thay tháº¿ {len(placeholder_map)} phrasal verbs báº±ng token.")
# === PhÃ¢n bÃ i há»c + tÃ¡ch tá»« vá»±ng ===
print("ğŸ“š Äang phÃ¢n tÃ­ch bÃ i há»c vÃ  tÃ¡ch tá»« vá»±ng...")

lines = text.splitlines()
output_rows = []
current_title = ""
seen = set()

for line in lines:
    line = line.strip()
    if not line:
        continue

    # Nháº­n diá»‡n tiÃªu Ä‘á» bÃ i há»c: báº¯t Ä‘áº§u báº±ng 1â€“3 chá»¯ sá»‘ + dáº¥u cháº¥m
    match = re.match(r'^(\d{1,3})\.\s*(.+)', line)
    if match:
        lesson_number = match.group(1)
        title = match.group(2).strip()
        current_title = f"{lesson_number}. {title}"
        output_rows.append(current_title.lower())  # dÃ²ng tiÃªu Ä‘á»
    else:
        # TÃ¬m cÃ¡c tá»« hoáº·c token trong dÃ²ng nÃ y
        words = re.findall(r"\b[a-zA-Z']+\b|__PHV\d+__", line)
        for word in words:
            if word.startswith("__PHV"):
                actual_phrase = placeholder_map.get(word, word)
                norm = normalize_phrasal(actual_phrase)
                if norm not in seen:
                    seen.add(norm)
                    output_rows.append(actual_phrase.lower())
            else:
                cleaned = word.strip("'")
                norm = normalize_phrasal(cleaned)
                if cleaned and norm not in seen:
                    seen.add(norm)
                    output_rows.append(cleaned.lower())


# === Xuáº¥t ra Excel ===
print(f"âœ… TrÃ­ch xuáº¥t Ä‘Æ°á»£c {len(output_rows)} dÃ²ng tá»« vá»±ng vÃ  tiÃªu Ä‘á».")
df = pd.DataFrame(output_rows, columns=["Vocabulary"])

df.to_excel(output_path, index=False)
print(f"ğŸ‰ HoÃ n táº¥t! File Ä‘Ã£ lÆ°u táº¡i: {output_path}")
